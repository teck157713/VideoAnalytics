{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "294aad44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\teck1\\AppData\\Local\\Temp\\ipykernel_26384\\2821860772.py:12: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use(\"seaborn\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import tensorflow\n",
    "import keras\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "555b7126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55ac27e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20171acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3908b559",
   "metadata": {},
   "outputs": [],
   "source": [
    "fightdir = \"C:/Projects/GitHub/ai/artifacts/data_ingestion/firstdata/fight\"\n",
    "nonfightdir = \"C:/Projects/GitHub/ai/artifacts/data_ingestion/firstdata/nofight\"\n",
    "#retrieve the list of video\n",
    "NonViolence_files_names_list = os.listdir(nonfightdir)\n",
    "Violence_files_names_list = os.listdir(fightdir)\n",
    "\n",
    "#Randomly select a video file from the Classes Directory.\n",
    "Random_NonViolence_Video = random.choice(NonViolence_files_names_list)\n",
    "Random_Violence_Video = random.choice(Violence_files_names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e52086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "imageheight, imagewidth = 64, 64\n",
    "seq = 16\n",
    "DATASET = \"C:/Projects/GitHub/ai/artifacts/data_ingestion/firstdata\"\n",
    "\n",
    "CLASSES_LIST = [\"nofight\", \"fight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a363522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_extraction(vid_path):\n",
    "  frames_list = []\n",
    "\n",
    "  #read vid file\n",
    "  video_reader = cv2.VideoCapture(vid_path)\n",
    "\n",
    "  #get total frames in vid\n",
    "  video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "  #calculate interval for frames to be added\n",
    "  skip_frames_window = max(int(video_frames_count/seq), 1)\n",
    "\n",
    "  for frame_counter in range(seq):\n",
    "    video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
    "\n",
    "    success, frame = video_reader.read()\n",
    "\n",
    "    if not success:\n",
    "      break\n",
    "\n",
    "    #resize\n",
    "    resized_frame = cv2.resize(frame, (imageheight, imagewidth))\n",
    "    normalize_frame = resized_frame / 255\n",
    "\n",
    "    frames_list.append(normalize_frame)\n",
    "\n",
    "  video_reader.release()\n",
    "\n",
    "  return frames_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4248d765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    features = []\n",
    "    labels = []\n",
    "    video_files_paths = []\n",
    "    \n",
    "    for class_index, class_name in enumerate(CLASSES_LIST):\n",
    "        files_list = os.listdir(os.path.join(DATASET, class_name))\n",
    "        \n",
    "        for file_name in files_list:\n",
    "            #get directory of the class\n",
    "            video_file_path = os.path.join(DATASET, class_name, file_name)\n",
    "            # Extract the frames of the video file.\n",
    "            frames = frames_extraction(video_file_path)\n",
    "            if len(frames) == seq:\n",
    "                # Append the data to their repective lists.\n",
    "                features.append(frames)\n",
    "                labels.append(class_index)\n",
    "                video_files_paths.append(video_file_path)\n",
    "                \n",
    "    features = np.asarray(features)\n",
    "    labels = np.array(labels)  \n",
    "\n",
    "    return features, labels, video_files_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbc77897",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create the dataset.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m features, labels, video_files_paths \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataset\u001b[49m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the dataset.\n",
    "features, labels, video_files_paths = create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a876269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the extracted data\n",
    "np.save(\"features.npy\",features)\n",
    "np.save(\"labels.npy\",labels)\n",
    "np.save(\"video_files_paths.npy\",video_files_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e8ab790",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels, video_files_paths = np.load(\"../features.npy\") , np.load(\"../labels.npy\") ,  np.load(\"../video_files_paths.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768a031a",
   "metadata": {},
   "source": [
    "# # Split and Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccbb5f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert labels into one-hot-encoded vectors\n",
    "one_hot_encoded_labels = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ea9b2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Data into Train ( 90% ) and Test Set ( 10% ).\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels, test_size = 0.1, shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c052cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 16, 64, 64, 3) (1800, 2)\n",
      "(200, 16, 64, 64, 3) (200, 2)\n"
     ]
    }
   ],
   "source": [
    "print(features_train.shape,labels_train.shape)\n",
    "print(features_test.shape, labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d2bdbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "\n",
    "mobilenet = MobileNetV2( include_top=False , weights=\"imagenet\")\n",
    "\n",
    "#Fine-Tuning to make the last 20 layer trainable\n",
    "mobilenet.trainable=True\n",
    "\n",
    "for layer in mobilenet.layers[:-40]:\n",
    "  layer.trainable=False\n",
    "\n",
    "#mobilenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d559d616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    " \n",
    "    model = Sequential()\n",
    "\n",
    "    ########################################################################################################################\n",
    "    \n",
    "    #Specifying Input to match features shape\n",
    "    model.add(Input(shape = (seq, imageheight, imagewidth, 3)))\n",
    "    \n",
    "    # Passing mobilenet in the TimeDistributed layer to handle the sequence\n",
    "    model.add(TimeDistributed(mobilenet))\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "                                    \n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "    \n",
    "    lstm_fw = LSTM(units=32)\n",
    "    lstm_bw = LSTM(units=32, go_backwards = True)  \n",
    "\n",
    "    model.add(Bidirectional(lstm_fw, backward_layer = lstm_bw))\n",
    "    \n",
    "    # model.add(Dropout(0.25))\n",
    "\n",
    "    # model.add(Dense(256,activation='relu'))\n",
    "    # model.add(Dropout(0.25))\n",
    "\n",
    "    # model.add(Dense(128,activation='relu'))\n",
    "    # model.add(Dropout(0.25))\n",
    "\n",
    "    # model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32,activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    \n",
    "    model.add(Dense(len(CLASSES_LIST), activation = 'softmax'))\n",
    " \n",
    "    ########################################################################################################################\n",
    " \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b16be87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_6 (TimeDi  (None, 16, 2, 2, 1280)    2257984   \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 16, 2, 2, 1280)    0         \n",
      "                                                                 \n",
      " time_distributed_7 (TimeDi  (None, 16, 5120)          0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirecti  (None, 64)                1319168   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3595874 (13.72 MB)\n",
      "Trainable params: 3019426 (11.52 MB)\n",
      "Non-trainable params: 576448 (2.20 MB)\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "# Constructing the Model\n",
    "MoBiLSTM_model = create_model()\n",
    "\n",
    "# Plot the structure of the contructed LRCN model.\n",
    "plot_model(MoBiLSTM_model, to_file = 'MobBiLSTM_model_structure_plot.png', show_shapes = True, show_layer_names = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22ee0e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "180/180 [==============================] - 55s 269ms/step - loss: 0.7628 - accuracy: 0.4688 - val_loss: 0.6911 - val_accuracy: 0.5028 - lr: 0.0100\n",
      "Epoch 2/80\n",
      "180/180 [==============================] - 45s 251ms/step - loss: 0.7050 - accuracy: 0.5236 - val_loss: 0.6868 - val_accuracy: 0.5917 - lr: 0.0100\n",
      "Epoch 3/80\n",
      "180/180 [==============================] - 45s 251ms/step - loss: 0.7093 - accuracy: 0.5153 - val_loss: 0.6888 - val_accuracy: 0.5944 - lr: 0.0100\n",
      "Epoch 4/80\n",
      "180/180 [==============================] - 46s 255ms/step - loss: 0.7072 - accuracy: 0.5021 - val_loss: 0.6926 - val_accuracy: 0.5222 - lr: 0.0100\n",
      "Epoch 5/80\n",
      "180/180 [==============================] - 46s 256ms/step - loss: 0.6907 - accuracy: 0.5236 - val_loss: 0.6844 - val_accuracy: 0.6111 - lr: 0.0100\n",
      "Epoch 6/80\n",
      "180/180 [==============================] - 46s 255ms/step - loss: 0.6876 - accuracy: 0.5361 - val_loss: 0.6774 - val_accuracy: 0.6472 - lr: 0.0100\n",
      "Epoch 7/80\n",
      "180/180 [==============================] - 46s 257ms/step - loss: 0.6775 - accuracy: 0.5854 - val_loss: 0.6716 - val_accuracy: 0.6333 - lr: 0.0100\n",
      "Epoch 8/80\n",
      "180/180 [==============================] - 48s 268ms/step - loss: 0.6683 - accuracy: 0.5903 - val_loss: 0.6691 - val_accuracy: 0.6083 - lr: 0.0100\n",
      "Epoch 9/80\n",
      "180/180 [==============================] - 46s 255ms/step - loss: 0.6633 - accuracy: 0.6111 - val_loss: 0.6619 - val_accuracy: 0.6472 - lr: 0.0100\n",
      "Epoch 10/80\n",
      "180/180 [==============================] - 46s 256ms/step - loss: 0.6567 - accuracy: 0.5944 - val_loss: 0.6528 - val_accuracy: 0.6639 - lr: 0.0100\n",
      "Epoch 11/80\n",
      "180/180 [==============================] - 46s 254ms/step - loss: 0.6517 - accuracy: 0.6222 - val_loss: 0.6437 - val_accuracy: 0.6722 - lr: 0.0100\n",
      "Epoch 12/80\n",
      "180/180 [==============================] - 48s 265ms/step - loss: 0.6432 - accuracy: 0.6465 - val_loss: 0.6371 - val_accuracy: 0.6639 - lr: 0.0100\n",
      "Epoch 13/80\n",
      "180/180 [==============================] - 48s 269ms/step - loss: 0.6220 - accuracy: 0.6687 - val_loss: 0.6361 - val_accuracy: 0.6583 - lr: 0.0100\n",
      "Epoch 14/80\n",
      "180/180 [==============================] - 48s 269ms/step - loss: 0.6060 - accuracy: 0.6944 - val_loss: 0.6156 - val_accuracy: 0.6667 - lr: 0.0100\n",
      "Epoch 15/80\n",
      "180/180 [==============================] - 47s 263ms/step - loss: 0.6023 - accuracy: 0.6854 - val_loss: 0.6323 - val_accuracy: 0.6389 - lr: 0.0100\n",
      "Epoch 16/80\n",
      "180/180 [==============================] - 46s 257ms/step - loss: 0.5762 - accuracy: 0.7021 - val_loss: 0.6304 - val_accuracy: 0.6444 - lr: 0.0100\n",
      "Epoch 17/80\n",
      "180/180 [==============================] - 47s 259ms/step - loss: 0.5813 - accuracy: 0.7118 - val_loss: 0.6271 - val_accuracy: 0.6583 - lr: 0.0100\n",
      "Epoch 18/80\n",
      "180/180 [==============================] - 47s 259ms/step - loss: 0.5668 - accuracy: 0.7167 - val_loss: 0.6240 - val_accuracy: 0.6694 - lr: 0.0100\n",
      "Epoch 19/80\n",
      "180/180 [==============================] - ETA: 0s - loss: 0.5288 - accuracy: 0.7396\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.005999999865889549.\n",
      "180/180 [==============================] - 46s 257ms/step - loss: 0.5288 - accuracy: 0.7396 - val_loss: 0.6312 - val_accuracy: 0.6750 - lr: 0.0100\n",
      "Epoch 20/80\n",
      "180/180 [==============================] - 46s 256ms/step - loss: 0.5302 - accuracy: 0.7479 - val_loss: 0.6341 - val_accuracy: 0.6583 - lr: 0.0060\n",
      "Epoch 21/80\n",
      "180/180 [==============================] - 46s 255ms/step - loss: 0.5084 - accuracy: 0.7611 - val_loss: 0.6049 - val_accuracy: 0.6861 - lr: 0.0060\n",
      "Epoch 22/80\n",
      "180/180 [==============================] - 46s 255ms/step - loss: 0.4939 - accuracy: 0.7653 - val_loss: 0.6269 - val_accuracy: 0.6583 - lr: 0.0060\n",
      "Epoch 23/80\n",
      "180/180 [==============================] - 46s 256ms/step - loss: 0.5151 - accuracy: 0.7590 - val_loss: 0.6332 - val_accuracy: 0.6639 - lr: 0.0060\n",
      "Epoch 24/80\n",
      "180/180 [==============================] - 46s 256ms/step - loss: 0.4840 - accuracy: 0.7875 - val_loss: 0.6061 - val_accuracy: 0.6833 - lr: 0.0060\n",
      "Epoch 25/80\n",
      "180/180 [==============================] - 47s 259ms/step - loss: 0.4432 - accuracy: 0.8097 - val_loss: 0.6071 - val_accuracy: 0.6694 - lr: 0.0060\n",
      "Epoch 26/80\n",
      "180/180 [==============================] - ETA: 0s - loss: 0.4568 - accuracy: 0.8007\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.003600000031292438.\n",
      "180/180 [==============================] - 46s 257ms/step - loss: 0.4568 - accuracy: 0.8007 - val_loss: 0.6438 - val_accuracy: 0.6889 - lr: 0.0060\n",
      "Epoch 27/80\n",
      "180/180 [==============================] - 46s 254ms/step - loss: 0.4303 - accuracy: 0.8208 - val_loss: 0.6632 - val_accuracy: 0.6917 - lr: 0.0036\n",
      "Epoch 28/80\n",
      "180/180 [==============================] - 46s 257ms/step - loss: 0.4294 - accuracy: 0.8257 - val_loss: 0.6814 - val_accuracy: 0.6750 - lr: 0.0036\n",
      "Epoch 29/80\n",
      "180/180 [==============================] - 49s 275ms/step - loss: 0.4077 - accuracy: 0.8271 - val_loss: 0.7010 - val_accuracy: 0.6722 - lr: 0.0036\n",
      "Epoch 30/80\n",
      "180/180 [==============================] - 53s 293ms/step - loss: 0.4206 - accuracy: 0.8229 - val_loss: 0.6899 - val_accuracy: 0.6667 - lr: 0.0036\n",
      "Epoch 31/80\n",
      "180/180 [==============================] - ETA: 0s - loss: 0.3740 - accuracy: 0.8507\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0021599999628961085.\n",
      "180/180 [==============================] - 50s 276ms/step - loss: 0.3740 - accuracy: 0.8507 - val_loss: 0.6765 - val_accuracy: 0.6667 - lr: 0.0036\n",
      "Epoch 32/80\n",
      "180/180 [==============================] - 44s 243ms/step - loss: 0.3698 - accuracy: 0.8576 - val_loss: 0.6781 - val_accuracy: 0.6889 - lr: 0.0022\n",
      "Epoch 33/80\n",
      "180/180 [==============================] - 43s 240ms/step - loss: 0.3616 - accuracy: 0.8583 - val_loss: 0.6714 - val_accuracy: 0.7139 - lr: 0.0022\n",
      "Epoch 34/80\n",
      "180/180 [==============================] - 43s 239ms/step - loss: 0.3524 - accuracy: 0.8736 - val_loss: 0.6826 - val_accuracy: 0.7167 - lr: 0.0022\n",
      "Epoch 35/80\n",
      "180/180 [==============================] - 44s 242ms/step - loss: 0.3557 - accuracy: 0.8569 - val_loss: 0.6737 - val_accuracy: 0.7056 - lr: 0.0022\n",
      "Epoch 36/80\n",
      "180/180 [==============================] - ETA: 0s - loss: 0.3257 - accuracy: 0.8792\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.0012959999497979878.\n",
      "180/180 [==============================] - 43s 241ms/step - loss: 0.3257 - accuracy: 0.8792 - val_loss: 0.6944 - val_accuracy: 0.7056 - lr: 0.0022\n",
      "Epoch 37/80\n",
      "180/180 [==============================] - 43s 242ms/step - loss: 0.3124 - accuracy: 0.8806 - val_loss: 0.6695 - val_accuracy: 0.7417 - lr: 0.0013\n",
      "Epoch 38/80\n",
      "180/180 [==============================] - 45s 249ms/step - loss: 0.3347 - accuracy: 0.8778 - val_loss: 0.6785 - val_accuracy: 0.7472 - lr: 0.0013\n",
      "Epoch 39/80\n",
      "180/180 [==============================] - 47s 259ms/step - loss: 0.3008 - accuracy: 0.8903 - val_loss: 0.7010 - val_accuracy: 0.7417 - lr: 0.0013\n",
      "Epoch 40/80\n",
      "180/180 [==============================] - 46s 258ms/step - loss: 0.2815 - accuracy: 0.8965 - val_loss: 0.7158 - val_accuracy: 0.7417 - lr: 0.0013\n",
      "Epoch 41/80\n",
      "180/180 [==============================] - ETA: 0s - loss: 0.2969 - accuracy: 0.8993\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0007775999838486314.\n",
      "180/180 [==============================] - 47s 261ms/step - loss: 0.2969 - accuracy: 0.8993 - val_loss: 0.7075 - val_accuracy: 0.7389 - lr: 0.0013\n",
      "Epoch 42/80\n",
      "180/180 [==============================] - 48s 264ms/step - loss: 0.2734 - accuracy: 0.9097 - val_loss: 0.7175 - val_accuracy: 0.7278 - lr: 7.7760e-04\n",
      "Epoch 43/80\n",
      "180/180 [==============================] - 47s 263ms/step - loss: 0.2676 - accuracy: 0.9035 - val_loss: 0.7153 - val_accuracy: 0.7278 - lr: 7.7760e-04\n",
      "Epoch 44/80\n",
      "180/180 [==============================] - 47s 262ms/step - loss: 0.2826 - accuracy: 0.9035 - val_loss: 0.7267 - val_accuracy: 0.7250 - lr: 7.7760e-04\n",
      "Epoch 45/80\n",
      "180/180 [==============================] - 47s 263ms/step - loss: 0.2546 - accuracy: 0.9125 - val_loss: 0.7500 - val_accuracy: 0.7250 - lr: 7.7760e-04\n",
      "Epoch 46/80\n",
      "180/180 [==============================] - ETA: 0s - loss: 0.2682 - accuracy: 0.9097\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.0004665599903091788.\n",
      "180/180 [==============================] - 47s 263ms/step - loss: 0.2682 - accuracy: 0.9097 - val_loss: 0.7153 - val_accuracy: 0.7472 - lr: 7.7760e-04\n",
      "Epoch 47/80\n",
      "180/180 [==============================] - 47s 260ms/step - loss: 0.2758 - accuracy: 0.9090 - val_loss: 0.7338 - val_accuracy: 0.7472 - lr: 4.6656e-04\n",
      "Epoch 48/80\n",
      "180/180 [==============================] - 46s 257ms/step - loss: 0.2416 - accuracy: 0.9194 - val_loss: 0.7434 - val_accuracy: 0.7444 - lr: 4.6656e-04\n"
     ]
    }
   ],
   "source": [
    "# Create Early Stopping Callback to monitor the accuracy\n",
    "early_stopping_callback = EarlyStopping(monitor = 'val_accuracy', patience = 10, restore_best_weights = True)\n",
    "\n",
    "# Create ReduceLROnPlateau Callback to reduce overfitting by decreasing learning\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                  factor=0.6,\n",
    "                                                  patience=5,\n",
    "                                                  min_lr=0.00001,\n",
    "                                                  verbose=1)\n",
    " \n",
    "# Compiling the model \n",
    "MoBiLSTM_model.compile(loss = 'categorical_crossentropy', optimizer = 'sgd', metrics = [\"accuracy\"])\n",
    " \n",
    "# Fitting the model \n",
    "MobBiLSTM_model_history = MoBiLSTM_model.fit(x = features_train, y = labels_train, epochs = 80, batch_size = 8 ,\n",
    "                                             shuffle = True, validation_split = 0.2, callbacks = [early_stopping_callback,reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23e785f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 4s 294ms/step - loss: 0.6971 - accuracy: 0.7150\n"
     ]
    }
   ],
   "source": [
    "model_evaluation_history = MoBiLSTM_model.evaluate(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3153593f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "MoBiLSTM_model.save('artifacts/training/model_v2.h5', include_optimizer=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
