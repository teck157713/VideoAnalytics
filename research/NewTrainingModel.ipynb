{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "294aad44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import tensorflow\n",
    "import keras\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "555b7126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55ac27e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20171acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3908b559",
   "metadata": {},
   "outputs": [],
   "source": [
    "fightdir = \"D:/Python Scripts/OpenCV/Data/firstdata/fight\"\n",
    "nonfightdir = \"D:/Python Scripts/OpenCV/Data/firstdata/nofight\"\n",
    "#retrieve the list of video\n",
    "NonViolence_files_names_list = os.listdir(nonfightdir)\n",
    "Violence_files_names_list = os.listdir(fightdir)\n",
    "\n",
    "#Randomly select a video file from the Classes Directory.\n",
    "Random_NonViolence_Video = random.choice(NonViolence_files_names_list)\n",
    "Random_Violence_Video = random.choice(Violence_files_names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e52086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "imageheight, imagewidth = 64, 64\n",
    "seq = 16\n",
    "DATASET = \"D:/Python Scripts/OpenCV/Data/firstdata\"\n",
    "\n",
    "CLASSES_LIST = [\"nofight\", \"fight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a363522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_extraction(vid_path):\n",
    "  frames_list = []\n",
    "\n",
    "  #read vid file\n",
    "  video_reader = cv2.VideoCapture(vid_path)\n",
    "\n",
    "  #get total frames in vid\n",
    "  video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "  #calculate interval for frames to be added\n",
    "  skip_frames_window = max(int(video_frames_count/seq), 1)\n",
    "\n",
    "  for frame_counter in range(seq):\n",
    "    video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
    "\n",
    "    success, frame = video_reader.read()\n",
    "\n",
    "    if not success:\n",
    "      break\n",
    "\n",
    "    #resize\n",
    "    resized_frame = cv2.resize(frame, (imageheight, imagewidth))\n",
    "    normalize_frame = resized_frame / 255\n",
    "\n",
    "    frames_list.append(normalize_frame)\n",
    "\n",
    "  video_reader.release()\n",
    "\n",
    "  return frames_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4248d765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    features = []\n",
    "    labels = []\n",
    "    video_files_paths = []\n",
    "    \n",
    "    for class_index, class_name in enumerate(CLASSES_LIST):\n",
    "        files_list = os.listdir(os.path.join(DATASET, class_name))\n",
    "        \n",
    "        for file_name in files_list:\n",
    "            #get directory of the class\n",
    "            video_file_path = os.path.join(DATASET, class_name, file_name)\n",
    "            # Extract the frames of the video file.\n",
    "            frames = frames_extraction(video_file_path)\n",
    "            if len(frames) == seq:\n",
    "                # Append the data to their repective lists.\n",
    "                features.append(frames)\n",
    "                labels.append(class_index)\n",
    "                video_files_paths.append(video_file_path)\n",
    "                \n",
    "    features = np.asarray(features)\n",
    "    labels = np.array(labels)  \n",
    "\n",
    "    return features, labels, video_files_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbc77897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset.\n",
    "features, labels, video_files_paths = create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a876269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the extracted data\n",
    "np.save(\"features.npy\",features)\n",
    "np.save(\"labels.npy\",labels)\n",
    "np.save(\"video_files_paths.npy\",video_files_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e8ab790",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels, video_files_paths = np.load(\"features.npy\") , np.load(\"labels.npy\") ,  np.load(\"video_files_paths.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768a031a",
   "metadata": {},
   "source": [
    "# # Split and Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccbb5f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert labels into one-hot-encoded vectors\n",
    "one_hot_encoded_labels = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ea9b2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Data into Train ( 90% ) and Test Set ( 10% ).\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels, test_size = 0.1, shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c052cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800, 16, 64, 64, 3) (1800, 2)\n",
      "(200, 16, 64, 64, 3) (200, 2)\n"
     ]
    }
   ],
   "source": [
    "print(features_train.shape,labels_train.shape)\n",
    "print(features_test.shape, labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d2bdbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "\n",
    "mobilenet = MobileNetV2( include_top=False , weights=\"imagenet\")\n",
    "\n",
    "#Fine-Tuning to make the last 40 layer trainable\n",
    "mobilenet.trainable=True\n",
    "\n",
    "for layer in mobilenet.layers[:-40]:\n",
    "  layer.trainable=False\n",
    "\n",
    "#mobilenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d559d616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    " \n",
    "    model = Sequential()\n",
    "\n",
    "    ########################################################################################################################\n",
    "    \n",
    "    #Specifying Input to match features shape\n",
    "    model.add(Input(shape = (seq, imageheight, imagewidth, 3)))\n",
    "    \n",
    "    # Passing mobilenet in the TimeDistributed layer to handle the sequence\n",
    "    model.add(TimeDistributed(mobilenet))\n",
    "    \n",
    "    model.add(Dropout(0.25))\n",
    "                                    \n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "    \n",
    "    lstm_fw = LSTM(units=32)\n",
    "    lstm_bw = LSTM(units=32, go_backwards = True)  \n",
    "\n",
    "    model.add(Bidirectional(lstm_fw, backward_layer = lstm_bw))\n",
    "    \n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(32,activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    \n",
    "    model.add(Dense(len(CLASSES_LIST), activation = 'softmax'))\n",
    " \n",
    "    ########################################################################################################################\n",
    " \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b16be87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDist  (None, 16, 2, 2, 1280)    2257984   \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 2, 2, 1280)    0         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDi  (None, 16, 5120)          0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 64)                1319168   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               16640     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3637090 (13.87 MB)\n",
      "Trainable params: 3060642 (11.68 MB)\n",
      "Non-trainable params: 576448 (2.20 MB)\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "# Constructing the Model\n",
    "MoBiLSTM_model = create_model()\n",
    "\n",
    "# Plot the structure of the contructed LRCN model.\n",
    "plot_model(MoBiLSTM_model, to_file = 'MobBiLSTM_model_structure_plot.png', show_shapes = True, show_layer_names = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ee0e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Early Stopping Callback to monitor the accuracy\n",
    "early_stopping_callback = EarlyStopping(monitor = 'val_accuracy', patience = 10, restore_best_weights = True)\n",
    "\n",
    "# Create ReduceLROnPlateau Callback to reduce overfitting by decreasing learning\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                  factor=0.6,\n",
    "                                                  patience=5,\n",
    "                                                  min_lr=0.00001,\n",
    "                                                  verbose=1)\n",
    " \n",
    "# Compiling the model \n",
    "MoBiLSTM_model.compile(loss = 'categorical_crossentropy', optimizer = 'sgd', metrics = [\"accuracy\"])\n",
    " \n",
    "# Fitting the model \n",
    "MobBiLSTM_model_history = MoBiLSTM_model.fit(x = features_train, y = labels_train, epochs = 50, batch_size = 8 ,\n",
    "                                             shuffle = True, validation_split = 0.2, callbacks = [early_stopping_callback,reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23e785f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 4s 325ms/step - loss: 0.9433 - accuracy: 0.7250\n"
     ]
    }
   ],
   "source": [
    "model_evaluation_history = MoBiLSTM_model.evaluate(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3153593f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DeDe\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "MoBiLSTM_model.save('210512_MobileNet_model_epoch30.h5', include_optimizer=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda37",
   "language": "python",
   "name": "cuda37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
